# GPT-2 Transformer æ¶æ„è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜GPT-2 Transformerçš„æ¶æ„è®¾è®¡å’Œå®ç°åŸç†ã€‚

## ğŸ“ æ•´ä½“æ¶æ„

```
è¾“å…¥Token IDs
    â†“
Token Embedding (vocab_size â†’ d_model)
    â†“
+ Positional Encoding (å¯å­¦ä¹ çš„)
    â†“
[Transformer Block Ã— N]
    â†“
Final Layer Normalization
    â†“
Language Model Head (d_model â†’ vocab_size)
    â†“
è¾“å‡ºLogits
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. Token Embeddingï¼ˆè¯åµŒå…¥ï¼‰

**åŠŸèƒ½**ï¼šå°†ç¦»æ•£çš„token IDè½¬æ¢ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤º

**å®ç°**ï¼š
```javascript
embedding_matrix: (vocab_size Ã— d_model)
token_vector = embedding_matrix[token_id]
```

**å‚æ•°**ï¼š
- `vocabSize`: è¯æ±‡è¡¨å¤§å°ï¼ˆGPT-2ä¸º50257ï¼‰
- `dModel`: åµŒå…¥ç»´åº¦ï¼ˆé€šå¸¸ä¸º768ï¼‰

### 2. Positional Encodingï¼ˆä½ç½®ç¼–ç ï¼‰

**åŠŸèƒ½**ï¼šä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯

**GPT-2ä½¿ç”¨æ–¹å¼**ï¼šå¯å­¦ä¹ çš„ä½ç½®åµŒå…¥ï¼ˆLearned Positional Encodingï¼‰

**å®ç°**ï¼š
```javascript
position_embedding: (max_seq_len Ã— d_model)
final_embedding = token_embedding + position_embedding
```

**ç‰¹ç‚¹**ï¼š
- æ¯ä¸ªä½ç½®æœ‰ç‹¬ç«‹çš„å¯å­¦ä¹ å‘é‡
- ä¸token embeddingç›¸åŠ ï¼ˆä¸æ˜¯æ‹¼æ¥ï¼‰

### 3. Transformer Blockï¼ˆTransformerå—ï¼‰

GPT-2çš„æ ¸å¿ƒæ„å»ºå—ï¼ŒåŒ…å«ä¸¤ä¸ªå­å±‚ï¼š

#### 3.1 Multi-Head Self-Attentionï¼ˆå¤šå¤´è‡ªæ³¨æ„åŠ›ï¼‰

**æ¶æ„**ï¼š
```
è¾“å…¥ x (seq_len Ã— d_model)
    â†“
Layer Normalization (Pre-LN)
    â†“
Multi-Head Attention
    â”œâ”€ Head 1: Q, K, V â†’ Attention â†’ (seq_len Ã— d_v)
    â”œâ”€ Head 2: Q, K, V â†’ Attention â†’ (seq_len Ã— d_v)
    â”œâ”€ ...
    â””â”€ Head H: Q, K, V â†’ Attention â†’ (seq_len Ã— d_v)
    â†“
Concatenate Heads â†’ (seq_len Ã— d_model)
    â†“
Output Projection â†’ (seq_len Ã— d_model)
    â†“
+ Residual Connection
```

**æ³¨æ„åŠ›è®¡ç®—æµç¨‹**ï¼š
1. **è®¡ç®—Q, K, VçŸ©é˜µ**
   ```
   Q = XW_q  (seq_len Ã— d_k)
   K = XW_k  (seq_len Ã— d_k)
   V = XW_v  (seq_len Ã— d_v)
   ```

2. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**
   ```
   scores = QK^T / âˆšd_k  (seq_len Ã— seq_len)
   ```

3. **åº”ç”¨å› æœæ©ç **ï¼ˆGPT-2å…³é”®ç‰¹æ€§ï¼‰
   ```
   causal_mask: ä¸‹ä¸‰è§’ä¸º0ï¼Œä¸Šä¸‰è§’ä¸º-âˆ
   masked_scores = scores + causal_mask
   ```

4. **Softmaxå½’ä¸€åŒ–**
   ```
   attention_weights = softmax(masked_scores)
   ```

5. **åŠ æƒæ±‚å’Œ**
   ```
   output = attention_weights Ã— V  (seq_len Ã— d_v)
   ```

**å‚æ•°**ï¼š
- `numHeads`: æ³¨æ„åŠ›å¤´æ•°ï¼ˆGPT-2 Smallä¸º12ï¼‰
- `dK = dV = dModel / numHeads`: æ¯ä¸ªå¤´çš„ç»´åº¦

#### 3.2 Feed Forward Networkï¼ˆå‰é¦ˆç½‘ç»œï¼‰

**æ¶æ„**ï¼š
```
è¾“å…¥ x (seq_len Ã— d_model)
    â†“
Layer Normalization (Pre-LN)
    â†“
Linear(d_model â†’ d_ff)
    â†“
GELU Activation
    â†“
Linear(d_ff â†’ d_model)
    â†“
+ Residual Connection
```

**å…¬å¼**ï¼š
```
FFN(x) = GELU(xW1 + b1)W2 + b2
```

**å‚æ•°**ï¼š
- `dFF`: å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦ï¼ˆé€šå¸¸æ˜¯dModelçš„4å€ï¼‰

**GELUæ¿€æ´»å‡½æ•°**ï¼š
```
GELU(x) = 0.5x(1 + tanh(âˆš(2/Ï€)(x + 0.044715xÂ³)))
```

### 4. Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰

**å…¬å¼**ï¼š
```
LN(x) = Î³ * (x - Î¼) / (Ïƒ + Îµ) + Î²
```

**ç‰¹ç‚¹**ï¼š
- Pre-LNæ¶æ„ï¼šåœ¨å­å±‚ä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–
- å¯¹ç‰¹å¾ç»´åº¦ï¼ˆd_modelï¼‰è¿›è¡Œå½’ä¸€åŒ–
- å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°Î³å’Œåç§»å‚æ•°Î²

### 5. Language Model Headï¼ˆè¯­è¨€æ¨¡å‹å¤´ï¼‰

**åŠŸèƒ½**ï¼šå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°çš„logits

**å®ç°**ï¼š
```
logits = hidden_states Ã— W_lm + b_lm
logits: (seq_len Ã— vocab_size)
```

## ğŸ”„ æ•°æ®æµç¤ºä¾‹

å‡è®¾è¾“å…¥åºåˆ—é•¿åº¦ä¸º3ï¼Œd_model=768ï¼š

```
è¾“å…¥: [token_1, token_2, token_3]

1. Token Embedding:
   [768ç»´å‘é‡_1, 768ç»´å‘é‡_2, 768ç»´å‘é‡_3]

2. + Positional Encoding:
   [768ç»´å‘é‡_1, 768ç»´å‘é‡_2, 768ç»´å‘é‡_3]

3. Transformer Block 1:
   - Attention: æ¯ä¸ªä½ç½®å…³æ³¨æ‰€æœ‰ä½ç½®ï¼ˆå—å› æœæ©ç é™åˆ¶ï¼‰
   - FFN: ä½ç½®æ„ŸçŸ¥çš„å‰é¦ˆå˜æ¢
   è¾“å‡º: [768ç»´å‘é‡_1, 768ç»´å‘é‡_2, 768ç»´å‘é‡_3]

4. ... (é‡å¤Næ¬¡) ...

5. Final Layer Norm:
   [768ç»´å‘é‡_1, 768ç»´å‘é‡_2, 768ç»´å‘é‡_3]

6. Language Model Head:
   [50257ç»´logits_1, 50257ç»´logits_2, 50257ç»´logits_3]
```

## ğŸ¯ å…³é”®è®¾è®¡ç‰¹ç‚¹

### 1. Pre-LNæ¶æ„

GPT-2ä½¿ç”¨Pre-LNï¼ˆLayer Normåœ¨å­å±‚ä¹‹å‰ï¼‰ï¼Œè€Œä¸æ˜¯Post-LNï¼š

```
Pre-LN:  x â†’ LN â†’ Attention â†’ +x
Post-LN: x â†’ Attention â†’ LN â†’ +x
```

**ä¼˜åŠ¿**ï¼šè®­ç»ƒæ›´ç¨³å®šï¼Œæ¢¯åº¦æµåŠ¨æ›´å¥½

### 2. å› æœæ©ç ï¼ˆCausal Maskï¼‰

ç¡®ä¿æ¨¡å‹åªèƒ½çœ‹åˆ°å½“å‰ä½ç½®åŠä¹‹å‰çš„ä¿¡æ¯ï¼š

```
æ©ç çŸ©é˜µï¼ˆ3Ã—3ç¤ºä¾‹ï¼‰:
[ 0, -âˆ, -âˆ]
[ 0,  0, -âˆ]
[ 0,  0,  0]
```

### 3. æ®‹å·®è¿æ¥

æ¯ä¸ªå­å±‚éƒ½æœ‰æ®‹å·®è¿æ¥ï¼Œå¸®åŠ©æ¢¯åº¦æµåŠ¨ï¼š

```
output = input + sublayer(LN(input))
```

### 4. è‡ªå›å½’ç”Ÿæˆ

ç”Ÿæˆè¿‡ç¨‹æ˜¯é€æ­¥çš„ï¼š
1. ç»™å®šåˆå§‹tokenåºåˆ—
2. é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
3. å°†æ–°tokenåŠ å…¥åºåˆ—
4. é‡å¤æ­¥éª¤2-3

## ğŸ“Š æ¨¡å‹è§„æ¨¡

### GPT-2 Smallï¼ˆæœ¬å®ç°ç¤ºä¾‹ï¼‰
- **å‚æ•°æ•°é‡**: ~163M
- **dModel**: 768
- **numLayers**: 12
- **numHeads**: 12
- **dFF**: 3072

### GPT-2 Medium
- **å‚æ•°æ•°é‡**: ~345M
- **dModel**: 1024
- **numLayers**: 24
- **numHeads**: 16
- **dFF**: 4096

### GPT-2 Large
- **å‚æ•°æ•°é‡**: ~762M
- **dModel**: 1280
- **numLayers**: 36
- **numHeads**: 20
- **dFF**: 5120

## ğŸ” å®ç°ç»†èŠ‚

### æ•°å€¼ç¨³å®šæ€§

1. **Softmaxæ•°å€¼ç¨³å®šæ€§**
   ```javascript
   // å‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º
   exp(x - max(x)) / sum(exp(x - max(x)))
   ```

2. **Layer Normçš„eps**
   ```javascript
   // é˜²æ­¢é™¤é›¶
   std = sqrt(variance + eps)  // eps = 1e-5
   ```

### æƒé‡åˆå§‹åŒ–

ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼š
```javascript
limit = sqrt(6.0 / (fan_in + fan_out))
weight = random(-limit, limit)
```

### çŸ©é˜µç»´åº¦æ£€æŸ¥

æ‰€æœ‰çŸ©é˜µè¿ç®—éƒ½ç¡®ä¿ç»´åº¦åŒ¹é…ï¼š
- Q, K: (seq_len Ã— d_k)
- V: (seq_len Ã— d_v)
- Attention Weights: (seq_len Ã— seq_len)
- Output: (seq_len Ã— d_v) â†’ (seq_len Ã— d_model)

## ğŸ“ å­¦ä¹ è¦ç‚¹

1. **æ³¨æ„åŠ›æœºåˆ¶**ï¼šç†è§£Qã€Kã€Vçš„ä½œç”¨å’Œè®¡ç®—æµç¨‹
2. **å¤šå¤´æ³¨æ„åŠ›**ï¼šä¸ºä»€ä¹ˆéœ€è¦å¤šä¸ªå¤´ï¼Œå¦‚ä½•æ‹¼æ¥
3. **å› æœæ©ç **ï¼šè‡ªå›å½’æ¨¡å‹çš„å…³é”®
4. **æ®‹å·®è¿æ¥**ï¼šä¸ºä»€ä¹ˆéœ€è¦ï¼Œå¦‚ä½•å®ç°
5. **Layer Norm**ï¼šä¸Batch Normçš„åŒºåˆ«
6. **ä½ç½®ç¼–ç **ï¼šä¸ºä»€ä¹ˆéœ€è¦ï¼Œå¦‚ä½•æ·»åŠ 

## ğŸ“š å‚è€ƒèµ„æ–™

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - TransformeråŸå§‹è®ºæ–‡
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2è®ºæ–‡
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) - å¯è§†åŒ–è®²è§£

---

**æ³¨æ„**ï¼šæœ¬å®ç°ä¸“æ³¨äºæ•™å­¦å’ŒåŸç†ç†è§£ï¼Œå®é™…åº”ç”¨å»ºè®®ä½¿ç”¨æˆç†Ÿçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚

